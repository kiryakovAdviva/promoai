# --- START OF FILE search_engine.py (Refactored v3 - Added hybrid search) ---

import faiss
import numpy as np
import os
import json
import traceback
import sys
import re
from typing import List, Tuple, Dict, Any, Optional
from tqdm import tqdm

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ü—É—Ç–µ–π ---
CACHE_DIR = "data/cache"

# –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å run_embedder.py)
EMBEDDINGS_FILENAME = "embeddings.npy"
INDEXED_CHUNKS_FILENAME = "indexed_chunks.json"
FAISS_INDEX_FILENAME = "faiss_index.bin"

INDEXED_CHUNKS_PATH = os.path.join(CACHE_DIR, INDEXED_CHUNKS_FILENAME)
FAISS_INDEX_PATH = os.path.join(CACHE_DIR, FAISS_INDEX_FILENAME)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–∞–Ω–Ω—ã—Ö ---
faiss_index: Optional[faiss.Index] = None
indexed_chunks: List[Dict[str, Any]] = []
index_dimension: Optional[int] = None
is_initialized: bool = False

def _load_index_and_chunks() -> bool:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤."""
    global faiss_index, indexed_chunks, index_dimension, is_initialized

    if is_initialized: # –£–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ
        return True

    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞: –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤...")

    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ ---
    if not os.path.exists(FAISS_INDEX_PATH):
        sys.stderr.write(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –§–∞–π–ª FAISS –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {FAISS_INDEX_PATH}\n")
        sys.stderr.write("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç run_embedder.py –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è.\n")
        return False
    if not os.path.exists(INDEXED_CHUNKS_PATH):
        sys.stderr.write(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {INDEXED_CHUNKS_PATH}\n")
        sys.stderr.write("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç run_embedder.py –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è.\n")
        return False

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ ---
    try:
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ {FAISS_INDEX_PATH}...")
        faiss_index = faiss.read_index(FAISS_INDEX_PATH)
        index_dimension = faiss_index.d
        print(f"   ‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω (–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {index_dimension}, –ö–æ–ª-–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {faiss_index.ntotal}).")

        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ {INDEXED_CHUNKS_PATH}...")
        with open(INDEXED_CHUNKS_PATH, "r", encoding="utf-8") as f:
            indexed_chunks = json.load(f)
        print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (–ö–æ–ª-–≤–æ: {len(indexed_chunks)}).")

        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è ---
        if faiss_index.ntotal != len(indexed_chunks):
            sys.stderr.write("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ "
                             f"({faiss_index.ntotal}) –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ ({len(indexed_chunks)}).\n")
            sys.stderr.write("   –í–æ–∑–º–æ–∂–Ω–æ, –∏–Ω–¥–µ–∫—Å –∏–ª–∏ —Ñ–∞–π–ª —á–∞–Ω–∫–æ–≤ —É—Å—Ç–∞—Ä–µ–ª–∏. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∏—Ö (run_embedder.py).\n")
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–æ–∏—Å–∫
            faiss_index = None
            indexed_chunks = []
            index_dimension = None
            return False

        is_initialized = True
        print("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return True

    except Exception as e:
        sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ/–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {e}\n")
        traceback.print_exc()
        faiss_index = None
        indexed_chunks = []
        index_dimension = None
        return False

def semantic_search(query_vector: np.ndarray, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É.

    Args:
        query_vector (np.ndarray): –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π).
        top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

    Returns:
        list: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π [(chunk_dict, similarity_score), ...], –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏.
              –ò–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    global faiss_index, indexed_chunks, index_dimension, is_initialized

    # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ ---
    if not is_initialized:
        if not _load_index_and_chunks():
            # –ï—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–æ–∏—Å–∫ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω
            return []

    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---
    if faiss_index is None or index_dimension is None: # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        sys.stderr.write("‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.\n")
        return []
    if query_vector is None or query_vector.size == 0:
        sys.stderr.write("‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞.\n")
        return []

    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ ---
    try:
        # FAISS –æ–∂–∏–¥–∞–µ—Ç float32
        query_vector_f32 = query_vector.astype(np.float32)
        # FAISS –æ–∂–∏–¥–∞–µ—Ç 2D –º–∞—Å—Å–∏–≤ (batch_size, dim)
        if query_vector_f32.ndim == 1:
            query_vector_f32 = np.expand_dims(query_vector_f32, axis=0)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        query_dim = query_vector_f32.shape[1]
        if query_dim != index_dimension:
            sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ ({query_dim}) "
                             f"–Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –∏–Ω–¥–µ–∫—Å–∞ ({index_dimension}).\n")
            return []
    except Exception as e:
        sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è FAISS: {e}\n")
        traceback.print_exc()
        return []

    # --- –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ ---
    results_with_scores: List[Tuple[Dict[str, Any], float]] = []
    try:
        # print(f"üîç –ü–æ–∏—Å–∫ FAISS top_k={top_k}...") # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
        # index.search –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (scores) –∏ –∏–Ω–¥–µ–∫—Å—ã (indices)
        scores, indices = faiss_index.search(query_vector_f32, top_k)
        # print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã FAISS raw: indices={indices}, scores={scores}") # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥

        # scores[0] –∏ indices[0], —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å batch_size=1
        if len(indices) > 0 and len(scores) > 0:
            for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):
                if idx == -1: # FAISS –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–µ–Ω—å—à–µ k
                    break
                if 0 <= idx < len(indexed_chunks):
                    results_with_scores.append((indexed_chunks[idx], float(score)))
                else:
                    # –≠—Ç–∞ –æ—à–∏–±–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–Ω–∏–∫–∞—Ç—å –ø—Ä–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å–≤—è–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–∞–Ω–Ω—ã—Ö
                    sys.stderr.write(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü–æ–ª—É—á–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å ({idx}) –æ—Ç FAISS "
                                     f"(—Ä–∞–Ω–≥ {rank+1}). –ú–∞–∫—Å. –∏–Ω–¥–µ–∫—Å –≤ –¥–∞–Ω–Ω—ã—Ö: {len(indexed_chunks)-1}.\n")
        # print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results_with_scores)}") # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥

    except Exception as e:
         sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è FAISS –ø–æ–∏—Å–∫–∞: {e}\n")
         traceback.print_exc()
         return [] # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    # IndexFlatIP –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ. –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
    # —ç—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É. FAISS —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ —É–±—ã–≤–∞–Ω–∏—é.
    return results_with_scores

def keyword_search(query: str, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ —Ç–µ–∫—Å—Ç–µ —á–∞–Ω–∫–æ–≤.
    """
    if not is_initialized:
        if not _load_index_and_chunks():
            return []

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
    query_terms = set(re.findall(r'\w+', query.lower()))
    if not query_terms:
        return []

    results_with_scores: List[Tuple[Dict[str, Any], float]] = []
    
    for chunk in indexed_chunks:
        text = chunk.get('text', '').lower()
        if not text:
            continue
            
        # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤
        matches = sum(1 for term in query_terms if term in text)
        if matches > 0:
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π / –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Ä–º–∏–Ω–æ–≤
            score = matches / len(query_terms)
            results_with_scores.append((chunk, score))

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
    results_with_scores.sort(key=lambda x: x[1], reverse=True)
    return results_with_scores[:top_k]

def hybrid_search(query: str, query_vector: np.ndarray, top_k: int = 5, 
                 semantic_weight: float = 0.7) -> List[Tuple[Dict[str, Any], float]]:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –∏ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
    
    Args:
        query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        query_vector: –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        semantic_weight: –í–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0-1)
    """
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
    semantic_results = semantic_search(query_vector, top_k=top_k * 2)
    keyword_results = keyword_search(query, top_k=top_k * 2)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    combined_results: Dict[str, Tuple[Dict[str, Any], float]] = {}
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for chunk, score in semantic_results:
        chunk_id = chunk.get('id', '')
        if chunk_id:
            combined_results[chunk_id] = (chunk, score * semantic_weight)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for chunk, score in keyword_results:
        chunk_id = chunk.get('id', '')
        if chunk_id:
            if chunk_id in combined_results:
                # –ï—Å–ª–∏ —á–∞–Ω–∫ —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º —Å–∫–æ—Ä
                old_chunk, old_score = combined_results[chunk_id]
                combined_results[chunk_id] = (chunk, old_score + score * (1 - semantic_weight))
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                combined_results[chunk_id] = (chunk, score * (1 - semantic_weight))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
    final_results = sorted(combined_results.values(), key=lambda x: x[1], reverse=True)
    return final_results[:top_k]

def filter_search_results(results: List[Tuple[Dict[str, Any], float]], 
                         filters: Dict[str, Any]) -> List[Tuple[Dict[str, Any], float]]:
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        filters: –°–ª–æ–≤–∞—Ä—å —Å —É—Å–ª–æ–≤–∏—è–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ {–ø–æ–ª–µ: –∑–Ω–∞—á–µ–Ω–∏–µ}
    """
    if not filters:
        return results
        
    filtered_results = []
    for chunk, score in results:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —É—Å–ª–æ–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞
        if all(chunk.get(k) == v for k, v in filters.items()):
            filtered_results.append((chunk, score))
            
    return filtered_results

def evaluate_search_quality(query: str, results: List[Tuple[Dict[str, Any], float]]) -> Dict[str, float]:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞.
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    """
    if not results:
        return {
            'relevance_score': 0.0,
            'diversity_score': 0.0,
            'coverage_score': 0.0
        }
    
    # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä)
    relevance_score = sum(score for _, score in results) / len(results)
    
    # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
    sources = set(chunk.get('source', '') for chunk, _ in results)
    diversity_score = len(sources) / len(results)
    
    # –û—Ü–µ–Ω–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è (–¥–ª–∏–Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤)
    total_length = sum(len(chunk.get('text', '')) for chunk, _ in results)
    coverage_score = min(1.0, total_length / 1000)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
    
    return {
        'relevance_score': float(relevance_score),
        'diversity_score': float(diversity_score),
        'coverage_score': float(coverage_score)
    }

def update_index(new_chunks: List[Dict[str, Any]], 
                new_embeddings: np.ndarray) -> bool:
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        new_chunks: –ù–æ–≤—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        new_embeddings: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    global faiss_index, indexed_chunks
    
    if not is_initialized:
        if not _load_index_and_chunks():
            return False
            
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        if new_embeddings.shape[1] != index_dimension:
            sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({new_embeddings.shape[1]}) "
                            f"–Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –∏–Ω–¥–µ–∫—Å–∞ ({index_dimension}).\n")
            return False
            
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –≤ –∏–Ω–¥–µ–∫—Å
        if new_embeddings.flags['C_CONTIGUOUS']:
            faiss_index.add(new_embeddings.astype(np.float32))
        else:
            new_embeddings = np.ascontiguousarray(new_embeddings.astype(np.float32))
            faiss_index.add(new_embeddings)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤
        indexed_chunks.extend(new_chunks)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        faiss.write_index(faiss_index, FAISS_INDEX_PATH)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤
        with open(INDEXED_CHUNKS_PATH, "w", encoding="utf-8") as f:
            json.dump(indexed_chunks, f, indent=2, ensure_ascii=False, default=str)
            
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_chunks)} –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤.")
        return True
        
    except Exception as e:
        sys.stderr.write(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}\n")
        traceback.print_exc()
        return False

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ (–º–æ–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è) ---
def initialize_search_engine():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –∏–Ω–¥–µ–∫—Å–∞ –∏ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∑–∞—Ä–∞–Ω–µ–µ."""
    _load_index_and_chunks()

# --- END OF FILE search_engine.py ---