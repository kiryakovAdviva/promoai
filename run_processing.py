# --- START OF FILE run_processing.py (Refactored v3) ---
import os
import sys
import traceback
from typing import List, Dict, Any

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –≤–Ω—É—Ç—Ä–∏ –ø–∞–∫–µ—Ç–∞ document_processor
try:
    from document_processor.document_parser import parse_document, RawContentBlock
    from document_processor.chunker import SimpleRecursiveTextSplitter
    from document_processor.metadata_extractor import extract_metadata
    from document_processor.common_utils import clean_text, hash_chunk, save_chunks_json, format_table_to_markdown
    print("‚úÖ All modules imported successfully.")
except ImportError as e:
     sys.stderr.write(f"‚ùå Failed to import modules. Ensure they are in the correct path/package structure: {e}\n")
     sys.exit(1) # –í—ã—Ö–æ–¥–∏–º, –µ—Å–ª–∏ –±–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ config.py –∏–ª–∏ .env) ---
INPUT_DIR = "data/input"
OUTPUT_DIR = "data/output"
PROCESSED_JSON_FILENAME = "processed_chunks.json"
OUTPUT_PATH = os.path.join(OUTPUT_DIR, PROCESSED_JSON_FILENAME)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–µ—Ä–∞
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
SEPARATORS = ["\n\n", "\n", ". ", "? ", "! ", "; ", ", ", " ", ""] # –î–æ–±–∞–≤–∏–ª ; ,

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞
text_splitter = SimpleRecursiveTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=SEPARATORS
)

def process_single_document(file_path: str) -> List[Dict[str, Any]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç: –ø–∞—Ä—Å–∏—Ç, —á–∞–Ω–∫—É–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≥–æ—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    document_name = os.path.basename(file_path)
    processed_chunks: List[Dict[str, Any]] = []
    chunk_index_counter = 0 # –°–∫–≤–æ–∑–Ω–æ–π —Å—á–µ—Ç—á–∏–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞

    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ -> –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä RawContentBlock
        raw_content_generator = parse_document(file_path)

        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        for block in raw_content_generator:
            block_chunks: List[str] = []
            block_text_representation: str = ""
            base_source_info = block.source_info # –û–±—â–∞—è –∏–Ω—Ñ–∞ –æ –±–ª–æ–∫–µ

            try:
                # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ ---
                if block.type == 'text' and isinstance(block.content, str):
                    block_text_representation = block.content # –£–∂–µ –æ—á–∏—â–µ–Ω–æ –≤ –ø–∞—Ä—Å–µ—Ä–µ
                    # –ß–∞–Ω–∫—É–µ–º —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞
                    block_chunks = text_splitter.split_text(block_text_representation)

                # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (PDF/DOCX) ---
                elif block.type == 'table' and isinstance(block.content, list):
                    table_data = block.content
                    headers = base_source_info.get("headers", [])
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Markdown –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    block_text_representation = format_table_to_markdown(table_data, headers)
                    # –†–µ—à–∞–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —á–∞–Ω–∫–æ–≤–∞—Ç—å Markdown —Ç–∞–±–ª–∏—Ü—ã
                    # –ü–æ–∫–∞ –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –æ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è
                    if len(block_text_representation) <= CHUNK_SIZE * 1.5: # –ö–æ—ç—Ñ—Ñ. 1.5 –¥–ª—è –∑–∞–ø–∞—Å–∞
                         block_chunks = [block_text_representation]
                    else:
                         # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –±–æ–ª—å—à–∞—è, —á–∞–Ω–∫—É–µ–º –µ—ë Markdown –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç
                         # (–≠—Ç–æ –º–æ–∂–µ—Ç –Ω–∞—Ä—É—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ –ª—É—á—à–µ, —á–µ–º –æ–≥—Ä–æ–º–Ω—ã–π —á–∞–Ω–∫)
                         print(f"    ‚ö†Ô∏è Table Markdown is large ({len(block_text_representation)} chars), chunking as text...")
                         block_chunks = text_splitter.split_text(block_text_representation)

                # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫ Excel ---
                elif block.type == 'excel_row' and isinstance(block.content, dict):
                    row_data = block.content
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ "–∫–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ. –∫–ª—é—á2: –∑–Ω–∞—á–µ–Ω–∏–µ2."
                    text_parts = [f"{k}: {v}" for k, v in row_data.items()]
                    block_text_representation = ". ".join(text_parts) + "." if text_parts else "–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ Excel."
                    # –°—Ç—Ä–æ–∫—É Excel –æ–±—ã—á–Ω–æ –Ω–µ —á–∞–Ω–∫—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, —Å—á–∏—Ç–∞–µ–º –µ—ë –æ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º
                    block_chunks = [block_text_representation]

                else:
                    sys.stderr.write(f"  ‚ö†Ô∏è Unknown block type '{block.type}' or invalid content in '{document_name}'. Skipping block.\n")
                    continue

                # --- –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ ---
                for chunk_text in block_chunks:
                    if not chunk_text or not chunk_text.strip():
                        continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏

                    chunk_id = hash_chunk(chunk_text, document_name, chunk_index_counter)

                    # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–∞ ---
                    # –ü–µ—Ä–µ–¥–∞–µ–º –≤—Å—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    meta = extract_metadata(
                        chunk_text=chunk_text,
                        document_name=document_name,
                        source_type=f"{block.type}_chunk", # –£—Ç–æ—á–Ω—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        page_number=base_source_info.get("page_number"),
                        table_headers=base_source_info.get("headers") if block.type in ['table', 'excel_row'] else None,
                        table_data=block.content if block.type == 'table' else None,
                        excel_row_data=block.content if block.type == 'excel_row' else None,
                        document_hyperlinks=base_source_info.get("document_hyperlinks"),
                        current_heading=base_source_info.get("current_heading")
                    )
                    # –î–æ–±–∞–≤–ª—è–µ–º ID —á–∞–Ω–∫–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –º–µ—Ç—É –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
                    meta["chunk_id"] = chunk_id
                    meta["chunk_index_in_doc"] = chunk_index_counter

                    processed_chunks.append({
                        "id": chunk_id,
                        "text": chunk_text.strip(),
                        "meta": meta
                    })
                    chunk_index_counter += 1

            except Exception as e_block:
                 sys.stderr.write(f"  ‚ùå ERROR processing block ({block.type}) in '{document_name}': {e_block}\n")
                 traceback.print_exc(limit=1) # –ö—Ä–∞—Ç–∫–∏–π —Ç—Ä–µ–π—Å–±–µ–∫ –¥–ª—è –æ—à–∏–±–∫–∏ –±–ª–æ–∫–∞

    except Exception as e_doc:
        sys.stderr.write(f"‚ùå‚ùå CRITICAL ERROR processing document '{document_name}': {e_doc}\n")
        traceback.print_exc() # –ü–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–±–µ–∫ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return [] # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞

    print(f"‚úÖ Finished processing: {document_name}. Total chunks generated: {len(processed_chunks)}")
    return processed_chunks


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ INPUT_DIR.
    """
    all_processed_chunks: List[Dict[str, Any]] = []
    processed_files_count = 0
    skipped_files_count = 0

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—ã–≤–æ–¥–∞, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("-" * 60)
    print(f"üöÄ Starting document processing from directory: {INPUT_DIR}")
    print(f"‚û°Ô∏è Output will be saved to: {OUTPUT_PATH}")
    print(f"‚öôÔ∏è Chunk settings: size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    print("-" * 60)

    if not os.path.exists(INPUT_DIR):
        print(f"‚ùå Error: Input directory '{INPUT_DIR}' not found.")
        return

    try:
        files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]
    except FileNotFoundError:
        print(f"‚ùå Error: Cannot access input directory '{INPUT_DIR}'.")
        return

    if not files_to_process:
        print(f"‚ÑπÔ∏è No files found in '{INPUT_DIR}' to process.")
        return

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for filename in files_to_process:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç MS Office)
        if filename.startswith('~$') or filename.startswith('.'):
            print(f"\n‚ÑπÔ∏è Skipping temporary/hidden file: {filename}")
            skipped_files_count += 1
            continue

        file_path = os.path.join(INPUT_DIR, filename)
        print(f"\n--- Processing file: {filename} ---")

        try:
            # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_chunks = process_single_document(file_path)

            if document_chunks:
                all_processed_chunks.extend(document_chunks)
                processed_files_count += 1
            elif document_chunks == []: # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ (–±—ã–ª–∏ –æ—à–∏–±–∫–∏ –∏–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç/–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π)
                 print(f"‚ÑπÔ∏è No chunks generated for '{filename}'. It might be unsupported, empty, or processing failed.")
                 skipped_files_count += 1
            # –°–ª—É—á–∞–π None –Ω–µ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–Ω–∏–∫–∞—Ç—å, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            else:
                print(f"‚ö†Ô∏è Unexpected empty result for '{filename}'. Skipping.")
                skipped_files_count += 1

        except Exception as e:
            # –û—Ç–ª–æ–≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∞–π–ª–∞ (—Ö–æ—Ç—è process_single_document –¥–æ–ª–∂–Ω–∞ –∏—Ö –ª–æ–≤–∏—Ç—å)
            print(f"‚ùå‚ùå UNHANDLED CRITICAL ERROR during processing of '{filename}': {e}")
            traceback.print_exc()
            skipped_files_count += 1
        print(f"--- Finished file: {filename} ---")

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    print("\n" + "=" * 60)
    print("üìä Processing Summary:")
    print(f"  Processed files: {processed_files_count}")
    print(f"  Skipped/Failed files: {skipped_files_count}")
    print(f"  Total chunks generated: {len(all_processed_chunks)}")
    print("=" * 60)

    if all_processed_chunks:
        print(f"üíæ Saving all {len(all_processed_chunks)} chunks to {OUTPUT_PATH}...")
        save_chunks_json(all_processed_chunks, OUTPUT_PATH)
    else:
        print("\n‚ÑπÔ∏è No chunks were generated to save.")

    print("\nüéâ Document processing finished.")


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É stdout/stderr –≤ UTF-8 (–ø–æ–ª–µ–∑–Ω–æ –≤ Windows)
    # sys.stdout.reconfigure(encoding='utf-8')
    # sys.stderr.reconfigure(encoding='utf-8')
    main()
# --- END OF FILE run_processing.py ---